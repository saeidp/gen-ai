{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86f7eee-b3e6-4ab2-8501-17854a5a3e5b",
   "metadata": {},
   "source": [
    "# Task 2a: Text summarization with small files with Titan Text Premier\n",
    "\n",
    "\n",
    "In this notebook, you ingest a small string of text directly into the Amazon Bedrock API (using the Titan Text model) and instruct it to summarize the input text. You can apply this approach to summarize call transcripts, meeting transcripts, books, articles, blog posts, and other relevant content when the input text length is within the context size limits of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf0831-5d5f-4e39-a534-3b747ed1ead9",
   "metadata": {},
   "source": [
    "## Task 2a.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ba24e3-ea39-409d-9eba-01cc85c2e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "bedrock_client = session.client('bedrock-runtime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125a62f-16a1-4758-9d71-c072eae1975c",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2a.2: Writing prompt with text to be summarized\n",
    "\n",
    "In this task, you use a short passage of text with fewer tokens than the maximum length supported by the foundation model. As a sample input text for this lab, you use a paragraph from an [AWS blog post](https://aws.amazon.com/jp/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/) announcing Amazon Bedrock.\n",
    "\n",
    "The prompt starts with an instruction `Please provide a summary of the following text.`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190e20e-77c5-430f-bda2-6573c284df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "\n",
    "Please provide a summary of the following text:\n",
    "\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we’re also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock’s serverless experience, \\\n",
    "customers can easily find the right model for what they’re trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aaa8f-759e-434a-883c-babb918f907a",
   "metadata": {},
   "source": [
    "## Task 2a.3: Creating request body with prompt and inference parameters \n",
    "\n",
    "In this task, you create the request body with the above prompt and inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630a1b1e-5def-4ef2-8019-d26c87e06325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request body\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data, \n",
    "    \"textGenerationConfig\":{\n",
    "        \"maxTokenCount\":2048,\n",
    "        \"stopSequences\":[],\n",
    "        \"temperature\":0,\n",
    "        \"topP\":0.9\n",
    "        }\n",
    "    }) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea022b46-01fa-4eb0-8586-f9bc06bad3e9",
   "metadata": {},
   "source": [
    "## Task 2a.4: Invoke foundation model via Boto3\n",
    "\n",
    "In this task, you send an API request to Amazon Bedrock specifying the request parameters: `modelId`, `accept`, and `contentType`. Following the provided prompt, the foundation model in Amazon Bedrock then summarizes the input text.\n",
    "\n",
    "### Complete Output Generation\n",
    "\n",
    "By default, the Amazon Bedrock service generates the entire summary for a given prompt in a single output. This can be slow if the model output contains many tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a692e9-b6db-48a1-8770-bac8708352bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Bedrock is a new service that makes AI-based applications using FMs accessible via an API. It offers a scalable, reliable, and secure AWS managed service with serverless experience, allowing customers to easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications. It includes Amazons Titan FMs, which consist of two new LLMs.\n"
     ]
    }
   ],
   "source": [
    "#model configuration and invoke the model\n",
    "modelId = 'amazon.titan-text-express-v1' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "outputText = \"\\n\"\n",
    "\n",
    "try:\n",
    "\n",
    "    response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    \n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "           print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                 \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                 \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        \n",
    "    else:\n",
    "        raise error\n",
    "\n",
    "print(outputText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487ffa8-d3e1-41fd-ab46-00ad609e0e18",
   "metadata": {},
   "source": [
    "### Streaming Output Generation\n",
    "\n",
    "Next, you explore how to use Amazon Bedrock's invoke_model_with_response_stream API to stream model outputs so users can consume outputs as they are generated. Rather than generating the full output at once, this API returns a ResponseStream that sends smaller output chunks from the model as they are produced. You can display these streaming outputs in a continuous, consumable view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d32cba-0bc5-4686-b7ef-51add0ec4da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': {'bytes': b'{\"outputText\":\"Amazon Bedrock is a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. It offers a scalable, reliable, and secure AWS managed service with serverless experie\",\"index\":0,\"totalOutputTextTokenCount\":null,\"completionReason\":null,\"inputTextTokenCount\":232}'}},\n",
       " {'chunk': {'bytes': b'{\"outputText\":\"nce, allowing customers to easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications. It also includes two new LLMs, Amazon Titan FMs.\",\"index\":0,\"totalOutputTextTokenCount\":93,\"completionReason\":\"FINISH\",\"inputTextTokenCount\":null,\"amazon-bedrock-invocationMetrics\":{\"inputTokenCount\":232,\"outputTokenCount\":93,\"invocationLatency\":5370,\"firstByteLatency\":4492}}'}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke model with response stream\n",
    "modelId = 'amazon.titan-text-express-v1'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "stream = response.get('body')\n",
    "output = list(stream)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adee53f4-320e-4bff-b2e8-22c61dcab3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown,Markdown,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704e4ae9-a17d-4ae9-a6a5-6021241af600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. It offers a scalable, reliable, and secure AWS managed service with serverless experience, allowing customers to easily find the right model, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications. It also includes two new LLMs, Amazon Titan FMs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelId = 'amazon.titan-text-express-v1'\n",
    "response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "stream = response.get('body')\n",
    "output = []\n",
    "i = 1\n",
    "if stream:\n",
    "    for event in stream:\n",
    "        chunk = event.get('chunk')\n",
    "        if chunk:\n",
    "            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "            text = chunk_obj['outputText']\n",
    "            clear_output(wait=True) # clear the output of current cell\n",
    "            output.append(text)\n",
    "            display_markdown(Markdown(''.join(output))) # convert string into markdowwn and renders in notebook\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44b514-578e-4964-9f42-729b2c98aa79",
   "metadata": {},
   "source": [
    "You have now experimented with using the boto3 SDK to access the Amazon Bedrock API. This SDK provides basic programmatic access to Bedrock capabilities. By leveraging this API, you were able to implement two use cases: 1) Generating an entire text summary of AWS news content at once, and 2) Streaming the summary output in chunks for incremental processing.\n",
    "\n",
    "### Try it yourself\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task2b.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2a822-137c-4e08-b116-d060ea717556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bedrock)",
   "language": "python",
   "name": "bedrock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
