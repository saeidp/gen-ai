{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b597e003-f3fb-4c21-8d1d-81a6274eeaf0",
   "metadata": {},
   "source": [
    "# Task 2b: Abstractive Text Summarization\n",
    "\n",
    "In this notebook, you manage challenges arising in large document summarization - input text can exceed model context lengths, generate hallucinated outputs, or trigger out-of-memory errors.\n",
    "\n",
    "To mitigate these issues, this notebook demonstrates an architecture using prompt chunking and chaining with the [LangChain](https://python.langchain.com/docs/get_started/introduction.html) framework, a toolkit enabling applications leveraging language models.\n",
    "\n",
    "You explore an approach addressing scenarios when user documents surpass token limits. Chunking splits documents into segments under context length thresholds before sequentially feeding them to models. This chains prompts across chunks, retaining prior context. You apply this approach to summarize call transcripts, meetings transcripts, books, articles, blog posts, and other relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4c4a1-c157-40ef-91f7-eb3d34d8804c",
   "metadata": {},
   "source": [
    "## Task 2b.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e13b09-f892-46a5-bbe1-38bc17387fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "bedrock_client = session.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d68b4-fe2b-4834-bfdc-6683276edc9a",
   "metadata": {},
   "source": [
    "## Task 2b.2: Summarize long text \n",
    "\n",
    "### Configuring LangChain with Boto3\n",
    "\n",
    "In this task, you need to specify the LLM for the LangChain Bedrock class, and can pass arguments for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150b874-d380-44d0-97a0-9ca33aec5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws import BedrockLLM\n",
    "modelId = \"amazon.titan-text-express-v1\"\n",
    "llm = BedrockLLM(\n",
    "    model_id=modelId,\n",
    "    model_kwargs={\n",
    "        \"maxTokenCount\": 2048,\n",
    "        \"temperature\": 0,\n",
    "        \"topP\": 1\n",
    "    },\n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e47dd-0e39-480b-8f2b-2411bec16f84",
   "metadata": {},
   "source": [
    "## Task 2b.3: Loading a text file with many tokens\n",
    "\n",
    "In this task, you can find a text file of [Amazon's CEO letter to shareholders in 2022](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders) in the letters directory. The following cell loads the text file and counts the number of tokens. You will see a warning indicating the number of tokens in the text file exceeds the maximum number of tokens for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe4259-6e25-42ff-a364-1d0d43024cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens\n",
    "shareholder_letter = \"2022-letter.txt\"\n",
    "\n",
    "with open(shareholder_letter, \"r\") as file:\n",
    "    letter = file.read()\n",
    "    \n",
    "llm.get_num_tokens(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8325da1-0cb6-458e-b9fb-be3889a4f18f",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** You can safely ignore the warnings and proceed to next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ceacba-2b61-4165-a880-9208a4d42f98",
   "metadata": {},
   "source": [
    "## Task 2b.4: Splitting the long text into chunks\n",
    "\n",
    "In this task, you split the text into smaller chunks because it is too long to fit in the prompt. `RecursiveCharacterTextSplitter` in LangChain supports splitting long text into chunks recursively until the size of each chunk becomes smaller than `chunk_size`. A text is separated with `separators=[\"\\n\\n\", \"\\n\"]` into chunks, which avoids splitting each paragraph into multiple chunks.\n",
    "\n",
    "Using 6,000 characters per chunk, you can get summaries for each portion separately. The number of tokens, or word pieces, in a chunk depends on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad22a7-d96b-4a6b-92d1-b440ecf45e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\"], chunk_size=4000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([letter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b74c2-015d-45d3-956f-d59c55d1dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(docs)\n",
    "\n",
    "num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "print(\n",
    "    f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1843708-6cb8-4f1c-89a2-cc5bb4bcf0f7",
   "metadata": {},
   "source": [
    "## Task 2b.5: Summarizing chunks and combining them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5dc5a-92de-402b-bb04-d42daa949ac0",
   "metadata": {},
   "source": [
    "In this task, assuming that the number of tokens is consistent in the other documents, you should be good to go. You can use LangChain's `load_summarize_chain` to summarize the text. `load_summarize_chain` provides three ways of summarization: `stuff`, `map_reduce`, and `refine`.\n",
    "\n",
    "- `stuff`: puts all the chunks into one prompt. Thus, this would hit the maximum limit of tokens.\n",
    "- `map_reduce`: summarizes each chunk, combines the summaries, and summarizes the combined summary. If the combined summary is too large, it would raise an error.\n",
    "- `refine`: summarizes the first chunk, and then summarizes the second chunk with the first summary. The same process repeats until all chunks are summarized.\n",
    "\n",
    "Both map_reduce and refine invoke the LLM multiple times and take time for obtaining the final summary. You can try map_reduce here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e82698-df7a-4324-851d-01dbdd6f8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set verbose=True if you want to see the prompts being used\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8fa47-bd02-40ef-9fa5-01b6cd496d94",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Depending on your number of documents, Bedrock request rate quota, and configured retry settings - the chain below may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbf3b5-7e95-4cb6-9937-55ee3f201433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke chain\n",
    "output = \"\"\n",
    "try:\n",
    "    \n",
    "    output = summary_chain.invoke(docs)\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d50ae-6d6f-4ec0-ad74-bc1a7f0150f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print output\n",
    "print(output['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66980d04-0eb6-4d0a-893f-7c09919d08d1",
   "metadata": {},
   "source": [
    "You have now experimented with using prompt chunking and chaining with the LangChain framework to summarize large documents while mitigating issues arising from long input text.\n",
    "\n",
    "### Try it yourself\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13be03e-f391-4a9a-90ad-c4ecd50752d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bedrock)",
   "language": "python",
   "name": "bedrock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
