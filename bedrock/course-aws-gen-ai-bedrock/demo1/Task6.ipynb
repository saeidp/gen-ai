{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adba6786",
   "metadata": {},
   "source": [
    "# Task 6: Bedrock model integration with Langchain Agents\n",
    "\n",
    "In this notebook, you learn how to use a plan-and-execute agent that determines the order of actions and implements them using the tools available to the agents. \n",
    "\n",
    "Certain applications demand an adaptable sequence of calls to the language models and various utilities to answer a user's question. The Langchain Agent interface is flexible and can integrate external tools with LLM's reasoning. Agents can select the tool to use based on the user input. Agents are capable of using multiple tools and utilizing the output of one tool as the input for the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af1034",
   "metadata": {},
   "source": [
    "## Task 6.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bd8701-e959-401b-b066-2ad2acea9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a service client by name using the default session.\n",
    "import math\n",
    "import numexpr\n",
    "import json\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name='ap-southeast-2')\n",
    "# model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_id=\"anthropic.claude-3-haiku-20240307-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38be8de",
   "metadata": {},
   "source": [
    "Next, you create an instance of LangChain's ChatBedrock class, which allows you to interact with a conversational AI model hosted on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a1ddab-343c-46a2-b2e9-d05550981474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of the ChatBedrock\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model=ChatBedrock(\n",
    "    model_id=model_id , \n",
    "    client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4063c43-3120-44a1-b126-a288696179a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AWS (Amazon Web Services) is a comprehensive cloud computing platform offered by Amazon that provides a wide range of services, including computing, storage, networking, databases, analytics, and more, to individuals, companies, and organizations of all sizes.', additional_kwargs={'usage': {'prompt_tokens': 18, 'completion_tokens': 51, 'total_tokens': 69}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, response_metadata={'usage': {'prompt_tokens': 18, 'completion_tokens': 51, 'total_tokens': 69}, 'stop_reason': 'end_turn', 'thinking': {}, 'model_id': 'anthropic.claude-3-haiku-20240307-v1:0'}, id='run-cf9d0518-e256-4433-9655-0c37d297c1ab-0', usage_metadata={'input_tokens': 18, 'output_tokens': 51, 'total_tokens': 69})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke model\n",
    "chat_model.invoke(\"what is AWS? Answer in a single senetence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef24bea",
   "metadata": {},
   "source": [
    "## Task 6.2: Synergizing Reasoning and Acting in Language Models Framework\n",
    "\n",
    "In this task, the ReAct framework enables large language models to interact with external tools to obtain additional information that results in more accurate and fact-based responses.\n",
    "\n",
    "Large language models can generate both explanations for their reasoning and task-specific responses in an alternating fashion.\n",
    "\n",
    "Producing reasoning explanations enables the models to infer, monitor, and revise action plans, and even handle unexpected scenarios. The action step allows the models to interface with and obtain information from external sources such as knowledge bases or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e54d64-a766-4303-9aff-a73f0a87c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1defc7f-4475-4071-9ab3-ff5806d068d1",
   "metadata": {},
   "source": [
    "In the next cell, you define a function `get_product_price` that serves as a tool within the Langchain framework and retrieves the price of the product specified in the query from `sales.csv` file created from previous task. It is a simple implementation to illustrate how tools can be designed to work with the Langchain framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6155c82-eb40-4eff-b5d5-d9fec4905278",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_product_price(query:str):\n",
    "    \"Useful when you need to lookup product price\"\n",
    "    import csv\n",
    "    prices = {}\n",
    "    try:\n",
    "        file=open('sales.csv', 'r')\n",
    "    except Exception as e:\n",
    "        return (\"Unable to look up the price for \" + query)\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        prices[row['product_id']] = row['price']\n",
    "    file.close()\n",
    "    qstr=query.split(\"\\n\")[0].strip()\n",
    "    try:\n",
    "            return (\"Price of product \"+qstr+\" is \"+prices.get(qstr)+\"\\n\")\n",
    "    except:\n",
    "            return (\"Price for product \"+qstr+\" is not avilable\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7ca6e-375f-432c-9539-8112ab8a1d5b",
   "metadata": {},
   "source": [
    "The following are an imaginary data that with the above function are created \n",
    "each row is like\n",
    "{\n",
    "    \"date\": \"2024-01-01\",\n",
    "    \"product_id\": \"P123\",\n",
    "    \"price\": \"19.99\",\n",
    "    \"units_sold\": \"5\"\n",
    "}\n",
    "\n",
    "\n",
    "and Prices\n",
    "{\n",
    "    \"P123\": \"19.99\",\n",
    "    \"P456\": \"12.49\",\n",
    "    \"P789\": \"7.99\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce2b82-438c-4b13-a941-74c79d6526c3",
   "metadata": {},
   "source": [
    "In the next cell, you define a function `calculator` that serves as a tool within the Langchain framework. This tool enables a language model to perform mathematical calculations by evaluating a given expression using Python's numexpr library. The tool is designed to handle cases where the expression is invalid. In that case, the tool aks the model to rethink its approach to the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c3256c-0806-4a19-ad42-4a2153f9ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Use this tool to solve a math problems that involve a single line mathematical expression.\n",
    "    Use math notation and not words. \n",
    "    Examples:\n",
    "        \"5*4\" for \"5 times 4\"\n",
    "        \"5/4\" for \"5 divided by 4\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(\n",
    "            numexpr.evaluate(\n",
    "            expression.strip(),\n",
    "            global_dict={},  \n",
    "            local_dict={} # add math constants, if needed\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return \"Rethink your approach to this calculation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ad97004-e7b8-4479-b966-2b276f8e55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_product_price, calculator]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a0ac1-d256-4188-b6b7-22a5f6960d34",
   "metadata": {},
   "source": [
    "In the next cell, you run helper functions to print trace output to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5321facb-cfc9-4e86-a669-db6dbdf116b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "def output_trace(element:str, trace, node=True):\n",
    "    global trace_handle\n",
    "    if trace_enabled:\n",
    "        print(datetime.datetime.now(),file=trace_handle)\n",
    "        print((\"Node: \" if node else \"Edge: \")+ element, file=trace_handle)\n",
    "        if element == \"ask_model_to_reason (entry)\":\n",
    "            for single_trace in trace:\n",
    "                print(single_trace, file=trace_handle)\n",
    "        else:\n",
    "            print(trace, file=trace_handle)\n",
    "        print('----', file=trace_handle)\n",
    "        \n",
    "def consolidate_tool_messages(message):\n",
    "    tool_messages=[]\n",
    "    for msg in message:\n",
    "        if isinstance(msg, ToolMessage):\n",
    "            tool_messages.append(msg)\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320f3b2",
   "metadata": {},
   "source": [
    "## Task 6.3: Building an Agent Graph\n",
    "\n",
    "In this task, you will be creating an agent graph for a conversational AI system that can interact with external tools. The agent graph is a state machine that defines the flow of the conversation and the interaction with the tools.\n",
    "\n",
    "In the next cell, you define nodes with associated functions that update the state based on input. Connect nodes using edges, where the graph transitions from one node to the next. Incorporate conditional edges to route the graph to different nodes based on specific conditions. Finally, compile the agent graph to prepare it for execution, handling transitions and state updates as defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b3001b5-f508-4e41-8c78-35d6e5455ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# ToolNode is a prebuilt component that runs the tool and appends the tool result to the messages \n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# let the model know the tools it can access\n",
    "model_with_tools = chat_model.bind_tools(tools)\n",
    "    \n",
    "# The following function acts as the conditional edge in the graph.\n",
    "# The next node could be the tools node or the end of the chain.\n",
    "def next_step(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        output_trace(\"next_step: Proceed to tools\",last_message, node=False)\n",
    "        return \"tools\"\n",
    "    output_trace(\"next_step: Proceed to end\",last_message, node=False)\n",
    "    return \"__end__\"\n",
    "\n",
    "#.The following node function invokes the model that has information about the available tools\n",
    "def ask_model_to_reason(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    output_trace(\"ask_model_to_reason (entry)\", consolidate_tool_messages(messages))\n",
    "    try:\n",
    "        response = model_with_tools.invoke(messages)\n",
    "    except Exception as e:\n",
    "        output_trace(\"ask_model_to_reason\", messages)\n",
    "        output_trace(\"ask_model_to_reason\", \"Exception: \"+str(e))\n",
    "        return {\"messages\": [messages.append(\"Unable to invoke the model\")]}\n",
    "    output_trace(\"ask_model_to_reason (exit)\", response)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "agent_graph = StateGraph(MessagesState)\n",
    "\n",
    "# Describe the nodes. \n",
    "# The first argument is the unique node name, and the second argument is the \n",
    "# function or object that will be called when the node is reached\n",
    "agent_graph.add_node(\"agent\", ask_model_to_reason)\n",
    "agent_graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Connect the entry node to the agent for the graph to start running\n",
    "agent_graph.add_edge(\"__start__\", \"agent\")\n",
    "\n",
    "# Once the graph transitions to the tools node, the graph will transition to the agent node\n",
    "agent_graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# The transition out of the agent node is conditional. \n",
    "# If the output from ask_model_to_reason function included a call to the tools, call the tool; \n",
    "# otherwise end the chain \n",
    "agent_graph.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    next_step,\n",
    ")\n",
    "\n",
    "# Compile the graph definition so that it can run\n",
    "\n",
    "react_agent = agent_graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ba496-f4e8-459c-a8e0-1add96724640",
   "metadata": {},
   "source": [
    "Next, you visualize the compiled graph. Observe the transition out of the agent node is conditional as indicated by the dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5fe11dc-4a20-4f82-82a0-7a2b21261ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(react_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12e190-6682-422b-a7e7-902baeab368f",
   "metadata": {},
   "source": [
    "In the next cell, you run helper function to print the graph output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d54e1a56-983e-4fb7-95e2-f1045d22bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a02d2-d337-40cd-ae33-52d168954345",
   "metadata": {},
   "source": [
    "Next, add one or more questions you want to ask the agent about product pricing from the sales.csv file you created in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f93542a4-00e1-4778-801f-54000bedd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of questions\n",
    "questions=[]\n",
    "questions.append(\"How much will it cost to buy 3 units of P002 and 5 units of P003?\")\n",
    "#questions.append(\"How many units of P010 can I buy with $200?\")\n",
    "#questions.append(\"Can I buy three units of P003 with $200? If not, how much more should I spend to get three units?\")\n",
    "#questions.append(\"Prices have gone up by 8%. How many units of P003 could I have purchased before the price increase with $140? How many can I buy after the price increase? Fractional units are not pssoible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2fdc8-7491-4b9b-a7b7-d4f7b909140b",
   "metadata": {},
   "source": [
    "To understand the steps involved in reasoning, enable trace. However, keep the trace output manageable by **commenting out all but one question** in the list above. Alternatively, you can disable trace and run all the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "286029b4-a16c-405c-9280-961c1e9378a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_enabled=True\n",
    "\n",
    "if trace_enabled:\n",
    "    file_name=\"trace_\"+str(datetime.datetime.now())+\".txt\"\n",
    "    trace_handle=open(file_name, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9e821-129a-4135-9969-6bc2bb7e2a1b",
   "metadata": {},
   "source": [
    "In the next cell, you invoke the agent with the questions from the list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba4fdd3-5e74-401a-8090-a3b95f68fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How much will it cost to buy 3 units of P002 and 5 units of P003?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, let's break this down step-by-step:\n",
      "Tool Calls:\n",
      "  get_product_price (toolu_bdrk_01FA7b6pvmZjrRtAzZcUSbx2)\n",
      " Call ID: toolu_bdrk_01FA7b6pvmZjrRtAzZcUSbx2\n",
      "  Args:\n",
      "    query: P002\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_product_price\n",
      "\n",
      "Price of product P002 is 60\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The price of product P002 is $60.\n",
      "Tool Calls:\n",
      "  get_product_price (toolu_bdrk_01VxQnpWj6RbnYkAMnadgzuW)\n",
      " Call ID: toolu_bdrk_01VxQnpWj6RbnYkAMnadgzuW\n",
      "  Args:\n",
      "    query: P003\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_product_price\n",
      "\n",
      "Price of product P003 is 70\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The price of product P003 is $70.\n",
      "\n",
      "To calculate the total cost:\n",
      "* 3 units of P002 at $60 each = 3 * $60 = $180\n",
      "* 5 units of P003 at $70 each = 5 * $70 = $350\n",
      "\n",
      "The total cost is $180 + $350 = $530.\n",
      "\n",
      "================================ Answer complete =================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_message=\"Answer the following questions as best you can. Do not make up an answer. Think step by step. Do not perform intermediate math calculations on your own. Use the calculator tool provided for math calculations.\"\n",
    "\n",
    "for q in questions:\n",
    "    inputs = {\"messages\": [(\"system\",system_message), (\"user\", q)]}\n",
    "    config={\"recursion_limit\": 15}\n",
    "    print_stream(react_agent.stream(inputs, config, stream_mode=\"values\"))\n",
    "    print(\"\\n\"+\"================================ Answer complete =================================\"+\"\\n\")\n",
    "\n",
    "if trace_enabled:\n",
    "    trace_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12383d8e",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:797620947747:studio-lifecycle-config/lcc-kernel"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
