{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa17f427-e19d-41d2-a41f-c1586573ee39",
   "metadata": {},
   "source": [
    "# Task 1b: Perform Text Generation using a prompt that includes Context\n",
    "In this notebook, you will learn how to generate an email response to a customer who was not happy with the quality of customer service they received from the customer support engineer. You will provide additional context to the model by including the contents of the actual email received from the unhappy customer.\n",
    "\n",
    "You will add more complexity with the help of PromptTemplates to leverage the LangChain framework for a similar use case. PromptTemplates allow you to create generic shells which can be populated with information later and obtain model outputs based on different scenarios.\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Language Models by chaining together various components to create advanced use cases.\n",
    "\n",
    "Due to the additional context in the prompt, the content produced in this notebook is of much better quality and relevance than the content produced earlier through zero-shot prompts. The prompt used in this notebook creates a custom LangChain prompt template for adding context to the text generation request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989730a-0939-4ce6-9f1f-0187b828da65",
   "metadata": {},
   "source": [
    "#### Scenario\n",
    "You are Bob, a Customer Service Manager at AnyCompany, and some of your customers are not happy with the customer service and are providing negative feedback on the service provided by customer support engineers. Now, you would like to respond to those customers humbly, apologizing for the poor service and regain their trust. You need the help of a Large Language Model (LLM) to generate a bulk of emails for you, which are human-friendly and personalized to the customer's sentiment from previous email correspondence.\n",
    "\n",
    "In this scenario, you can leverage the power of LangChain's PromptTemplates to create a generic shell for generating personalized email responses based on the customer's previous email. The PromptTemplate will incorporate the customer's original email content, allowing the LLM to understand the context and sentiment, and then generate a relevant and customized response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf7d109-e1b1-4337-9544-2136afe1d481",
   "metadata": {},
   "source": [
    "## Task 1b.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080bcb53-2c28-433f-a2b1-fee1ffc59f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "bedrock_client = session.client('bedrock-runtime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d2507-dbf7-463d-b989-4b2ea35171fa",
   "metadata": {},
   "source": [
    "## Task 1b.2: Invoke the Bedrock LLM Model\n",
    "\n",
    "In this task, you create an instance of the Bedrock class from llms. This expects a `model_id` which is the Amazon Resource Name (ARN) of the model available in Amazon Bedrock.\n",
    "\n",
    "Optionally, you can pass a previously created boto3 client as well as some `model_kwargs` which can hold parameters such as `temperature`, `top_p`, `max_token_count`, or `stop_sequences` (more information on parameters can be explored in the Amazon Bedrock console).\n",
    "\n",
    "Refer to [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) for Available text generation model Ids under Amazon Bedrock.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The different models support different `model_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afe6b461-405c-44d8-b9ef-f62330c542c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "# model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "model_kwargs =  { \n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1,\n",
    "}\n",
    "\n",
    "# LangChain class for chat\n",
    "chat_model = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a2cc3-e49f-400b-b73e-b55c8a02e885",
   "metadata": {},
   "source": [
    "## Task 1b.3: Create a LangChain custom prompt template\n",
    "\n",
    "In this task, you will create a template for the prompt that you can pass different input variables on every run. This is useful when you have to generate content with different input variables that you may be fetching from a database.\n",
    "\n",
    "In the previous task, we hardcoded the prompt. It might be the case that you have multiple customers sending similar negative feedback, and you now want to use each of those customers' emails and respond to them with an apology, but you also want to keep the response a bit personalized. In the following cell, you will explore how you can create a `PromptTemplate` to achieve this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09136346-93ad-4095-b813-29dd0f7de20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that has multiple input variables\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"customerServiceManager\", \"customerName\", \"feedbackFromCustomer\"], \n",
    "    template=\"\"\"\n",
    "\n",
    "Human: Create an apology email from the Service Manager {customerServiceManager} at AnyCompany to {customerName} in response to the following feedback that was received from the customer: \n",
    "<customer_feedback>\n",
    "{feedbackFromCustomer}\n",
    "</customer_feedback>\n",
    "\n",
    "Assistant:\"\"\"\n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(customerServiceManager=\"Bob Smith\", \n",
    "                                 customerName=\"John Doe\", \n",
    "                                 feedbackFromCustomer=\"\"\"Hello Bob,\n",
    "     I am very disappointed with the recent experience I had when I called your customer support.\n",
    "     I was expecting an immediate call back but it took three days for us to get a call back.\n",
    "     The first suggestion to fix the problem was incorrect. Ultimately the problem was fixed after three days.\n",
    "     We are very unhappy with the response provided and may consider taking our business elsewhere.\n",
    "     \"\"\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "510ae28b-8f63-46e4-8d58-be01b563733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prompt has 156 tokens\n"
     ]
    }
   ],
   "source": [
    "# get number of tokens\n",
    "num_tokens = chat_model.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddead720-c31a-45a8-94e4-758d952a5bc8",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:**  You can safely ignore the warnings and proceed to next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1427b81a-2455-4f65-b2fe-cfca7a4cceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke\n",
    "response = chat_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "721ce05e-7878-43be-80e1-773ff810f7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a draft of the apology email from Service Manager Bob Smith at AnyCompany to John Doe:\n",
      "\n",
      "Subject: Apology for Unsatisfactory Customer Service Experience\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I am writing to express my sincere apologies for the poor customer service experience you recently had when contacting our support team. I understand your frustration with the delayed response time, the incorrect initial suggestion, and the overall unsatisfactory resolution to your issue. This is not the level of service we strive to provide, and I am disappointed that we failed to meet your expectations.\n",
      "\n",
      "Please know that I have thoroughly reviewed this incident with our customer support team to identify the breakdowns that led to this unsatisfactory experience. I have also implemented additional training and process improvements to ensure this does not happen again in the future. Your business and satisfaction are extremely important to us, and we are committed to providing a much higher level of service going forward.\n",
      "\n",
      "As a gesture of goodwill, I would like to offer you a 20% discount on your next purchase with us. Please let me know if there is anything else I can do to regain your trust and confidence in AnyCompany.\n",
      "\n",
      "I sincerely apologize for the inconvenience and appreciate your patience and understanding.\n",
      "\n",
      "Sincerely,\n",
      "Bob Smith\n",
      "Service Manager\n",
      "AnyCompany\n"
     ]
    }
   ],
   "source": [
    "# Configure a Chain to parse output\n",
    "chain = StrOutputParser()\n",
    "formatted_response=chain.invoke(response)\n",
    "# print(response)\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b004e6-2eaf-4d32-a1ac-443e73351366",
   "metadata": {},
   "source": [
    "You have successfully learned that invoking the LLM without any context might not yield the desired results. By adding context and further using the prompt template to constrain the output from the LLM, you were able to successfully obtain your desired output.\n",
    "\n",
    "### Try it yourself\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with **Task 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c3e5a-d35e-464a-9020-100edb3d41a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bedrock)",
   "language": "python",
   "name": "bedrock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
